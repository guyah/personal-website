---
title: "Why your context window feels smaller than it is"
description: "Token budgets are an engineering constraint, not a vibes problem: composition, overflow rates, and a rule-of-thumb sanity check."
pubDate: 2026-02-02
tags: [llm, engineering, data-science]
icon: "üßæ"
---

A 32k context window sounds enormous‚Ä¶ until you build a system that uses it.

Then you discover the recurring failure mode:

- prompt templates grow
- conversation history grows
- retrieved documents grow
- guardrails grow

‚Ä¶and suddenly you‚Äôre truncating the one part you cared about.

This post is a reproducible way to think about token budgets as a **composition problem**.

Plots generated by:

- `site/analysis/generate_all.py` (task: `token_budgets`)

## Rule-of-thumb: chars per token

You can‚Äôt reason about budgets in characters. Tokenizers aren‚Äôt fixed-width.

But for rough planning, an English-ish heuristic is:

> **~4 characters per token** (including spaces/punctuation, averaged)

On real prompts (measured via API usage), the ratio clusters near ~4 (language-dependent):

![](/blog/2026-02-02-token-budgets/chars-per-token.svg)

This should not be used for billing or precision. It‚Äôs a *back-of-the-envelope planner*.

## The real issue: budget composition

What matters operationally isn‚Äôt ‚Äúdo we have 32k tokens?‚Äù

It‚Äôs:

- how many tokens are reserved for system/tooling
- how many go to history
- how many go to retrieval
- how many must remain for the model‚Äôs response

A (small-sample) prompt token-count distribution from the measurement set looks like this:

![](/blog/2026-02-02-token-budgets/prompt-size-hist.svg)

### The point of this plot

Even if your *average* prompt fits, you still need to manage the **overflow tail**.

Overflow is often triggered by:

- long ‚Äúhelp‚Äù pages retrieved by RAG
- multi-turn conversations with code blocks
- multi-document summarization tasks

## What I do in practice

1. **Set explicit budgets per component** (system/history/retrieval/response).
2. **Measure** the empirical distribution in production.
3. **Design for p95/p99**, not the mean.
4. **Compress the right thing**:
   - summarize old history instead of truncating the most recent user message
   - retrieve fewer docs but with higher precision
   - retrieve smaller chunks
5. **Fail loudly** when budgets are exceeded.

Silent truncation is the enemy of trust.

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a real-tokenizer comparison (e.g. tiktoken) once that‚Äôs available in this environment.
- Add an example of budget-aware RAG chunking.
