---
title: "Voice agent latency: measure the tail, not the mean"
description: "A latency playbook: pick user-facing targets, measure quantiles, and focus effort where the tail lives (with real measurements)."
pubDate: 2026-02-03
tags: [voice, systems]
icon: "⏱"
---

import Tldr from "../../components/Tldr.astro";
import Numbers from "../../components/Numbers.astro";
import Insights from "../../components/Insights.astro";
import Callout from "../../components/Callout.astro";

<Tldr
	items={[
		"Mean latency is a lie; p95/p99 is what users remember.",
		"Pick an explicit target (e.g., p95 < 1.5s) and design backwards.",
		"Measure first—even crude request-level timings beat vibes.",
	]}
/>

Voice agents feel human when turn-taking feels human.

That’s not philosophy; it’s a latency distribution statement.

- Users forgive an average delay.
- Users do *not* forgive a spiky p95/p99.

Plots generated by:

- `site/analysis/generate_all.py` (task: `voice_latency`)

## What we measured (real, small sample)

These plots are built from a small set of real API timing measurements (non-streaming request-level latency; not true token TTFT).

<Numbers
	rows={[
		["Median", "~1.25s"],
		["p90", "~1.54s"],
		["Notes", "Request-level timing; streaming TTFT will differ"],
	]}
/>

## Histogram: what the user feels

![](/blog/2026-02-03-voice-latency/end-to-first-audio-hist.svg)

## Quantiles: diagnose the tail

![](/blog/2026-02-03-voice-latency/component-quantiles.svg)

<Insights
	title="How to make this useful in production"
	items={[
		"Set a turn-level SLO: e.g., p50 < 900ms, p95 < 1500ms.",
		"Stamp events you control (endpointer, ASR final, LLM first token, TTS first audio).",
		"When p95 regresses, you need a component breakdown or you’ll thrash.",
	]}
/>

<Callout
	title="A blunt rule"
	body="If you can’t explain your p95 with component timings, you can’t fix it reliably. You’ll ship random optimizations and hope."
/>

## What I do when the tail is too fat

- **LLM tail dominates:** smaller model, caching, batching, speculative decoding, or change the UX (fillers, partial replies).
- **ASR tail dominates:** better endpointing, partial stability heuristics, or a different ASR stack.
- **TTS tail dominates:** streaming first chunk, faster vocoder path, reduce post-processing.

## Reproduce

```bash
python site/analysis/generate_all.py
```
