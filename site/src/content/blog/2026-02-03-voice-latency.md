---
title: "Voice agent latency: measure the tail, not the mean"
description: "A synthetic-but-realistic latency decomposition: why p95 dominates UX, and how to localize the long tail across ASR, LLM, and TTS."
pubDate: 2026-02-03
tags: [voice, systems, data-science]
icon: "⏱"
---

Voice agents feel human when turn-taking feels human.

That’s not a philosophical statement; it’s a latency distribution statement.

- Users forgive an average delay.
- Users do *not* forgive a spiky p95/p99.

This post shows a small analysis pattern I like: attribute tail latency to components.

Plots generated by:

- `site/analysis/generate_all.py` (task: `voice_latency`)

## Decompose “time to first audio”

One reasonable metric is:

> **end-to-first-audio**: from user stopping speech → first synthesized audio frame emitted.

You can decompose it into components:

- VAD hangover / endpointer delay
- ASR partial stability
- LLM time-to-first-token (TTFT)
- TTS time-to-first-audio (TTFA)
- output buffering

In a synthetic dataset (meant to resemble real logs), the end-to-first-audio histogram looks like:

![](/blog/2026-02-03-voice-latency/end-to-first-audio-hist.svg)

Even with “good” averages, there’s typically a fat right tail.

## Localize the tail with quantiles

A useful trick: plot **component latency at each quantile**.

If ASR dominates p50 but TTS dominates p95, you’ll know where to spend effort.

![](/blog/2026-02-03-voice-latency/component-quantiles.svg)

How to read this:

- left side (low quantiles): typical turns
- right side (high quantiles): the moments users remember

## A measurement checklist that scales

If you want this to work in production:

1. **Stamp events** at the boundaries you control:
   - endpointer fired
   - ASR partial/final timestamps
   - LLM first token received
   - TTS first PCM chunk received
   - audio playback started
2. **Join by a turn ID**.
3. Track p50/p95/p99 for each component.
4. Add **correlation**: which components co-spike?

## Engineering implications

- If the tail is dominated by LLM TTFT, you need batching, speculative decoding, smaller model, or cached responses.
- If the tail is dominated by ASR finalization, you likely need better endpointer + partial stability heuristics.
- If the tail is dominated by TTS, you may need a streaming vocoder path and smaller first-chunk generation.

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a “turn-taking simulator” to estimate interruption rate as a function of latency.
- Add a plot of barge-in events vs endpointer threshold.
