---
title: "Attention is a weighted average: a toy self-attention you can plot"
description: "Build intuition for transformers by visualizing softmax(QK·µÄ/‚àöd) and the entropy of attention rows."
pubDate: 2026-02-01
tags: [transformers, llm, math]
icon: "üß†"
---

A transformer layer can look like alien algebra until you see it as something mundane:

> **Self-attention produces weighted averages of value vectors, where weights come from softmax similarities between queries and keys.**

This post is a deliberately small ‚Äútoy attention‚Äù that you can reproduce, plot, and use to build the right mental model.

All plots are generated by:

- `site/analysis/generate_all.py` (task: `attention_toy`)

## The object we visualize

In a single-head self-attention block, the attention weights are:

\[
A = \mathrm{softmax}(QK^\top / \sqrt{d})
\]

- Each **row** corresponds to a query token.
- Each row is a probability distribution over key tokens.

## Plot 1 ‚Äî attention matrix

Here is a 12√ó12 attention matrix from a randomly-initialized toy setup:

![](/blog/2026-02-01-attention-toy/attention-heatmap.svg)

What to notice:

- Rows don‚Äôt have to be ‚Äúpeaky.‚Äù In early training or random init, attention can be diffuse.
- Some rows prefer a few columns. That‚Äôs the beginning of ‚Äútoken i attends to token j.‚Äù

## Plot 2 ‚Äî row entropy (how peaky is attention?)

A simple scalar summary per row is entropy:

- low entropy ‚áí one/few tokens dominate
- high entropy ‚áí attention is spread out

![](/blog/2026-02-01-attention-toy/attention-row-entropy.svg)

This is a helpful metric when debugging:

- If attention is *too peaky everywhere*, you can get brittle behavior.
- If attention is *too flat everywhere*, the model may fail to focus and behave like a bag-of-words smoother.

## The biggest misconception: attention ‚â† explanation

Even when attention is sharp, it‚Äôs not automatically a faithful explanation of the model‚Äôs reasoning.

Why?

- attention is one mechanism in a deep stack
- residual streams allow information to bypass a head
- multiple heads can distribute ‚Äúreasoning‚Äù across pathways

Still: plotting attention is useful for intuition, and sometimes for debugging.

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Extend this to multi-head attention and show how heads specialize.
- Add a toy causal mask and show how it changes the heatmap structure.
