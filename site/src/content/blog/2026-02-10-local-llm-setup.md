---
title: "Running an LLM locally: VRAM math, quantization, and the boring systems choices"
description: "A practical guide to choosing a GPU, estimating VRAM needs, picking GGUF/vLLM/TGI, and understanding throughput vs latency."
pubDate: 2026-02-10
tags: [systems, llm, inference, gpu, tooling]
icon: "üõ†Ô∏è"
draft: true
---

‚ÄúCan I run this model locally?‚Äù is almost never a model question.

It‚Äôs a **memory question**, then a **throughput question**, then an **ops question**.

This post is my high-signal checklist for getting to a local setup that‚Äôs:

- predictable
- debuggable
- fast enough

Plots generated by:

- `site/analysis/generate_all.py` (task: `local_llm_setup`)

## Step 0 ‚Äî decide your goal

Local LLM setups fall into three buckets:

1. **Interactive chat** (latency-sensitive)
2. **Batch jobs** (throughput-sensitive)
3. **Fine-tuning / LoRA** (memory + bandwidth-sensitive)

Pick one. Otherwise you end up with a setup that is mediocre at all three.

## Step 1 ‚Äî VRAM math you can do on a napkin

Your GPU memory goes to a few big buckets:

- **weights** (dominant, especially FP16)
- **KV cache** (dominant for long context + high concurrency)
- **runtime overhead** (activations, fragmentation, allocator slack, framework)

A rough breakdown for typical local setups:

![](/blog/2026-02-10-local-llm-setup/vram-breakdown.svg)

The useful takeaway:

- quantizing weights helps a lot
- but long context and concurrency can still eat you alive via KV cache

### KV cache scales linearly (and doesn‚Äôt care about your optimism)

If you remember one thing, remember this chart:

![](/blog/2026-02-10-local-llm-setup/kv-cache-vs-context.svg)

Why it matters:

- ‚ÄúI just want 32k context‚Äù is a memory commitment.
- ‚ÄúI want 16 concurrent chats‚Äù multiplies the KV cache budget.

For many local setups, the constraint isn‚Äôt FLOPs‚Äîit‚Äôs: *can you keep enough KV in VRAM without paging?*

## Step 2 ‚Äî picking a GPU (for local inference)

### Prioritize VRAM capacity first

For local inference, the ordering is usually:

1. **VRAM capacity** (does it fit?)
2. **memory bandwidth** (does it run fast?)
3. compute (tensor cores, etc.)

A slower GPU that fits the model is better than a faster one that forces CPU offload.

### Don‚Äôt ignore the boring constraints

- power/thermals (sustained clocks)
- physical space
- PSU headroom
- noise (seriously)

## Step 3 ‚Äî precision and quantization: what you‚Äôre trading

### FP16 / BF16

- simplest
- best compatibility
- expensive in VRAM

### INT8

- often a sweet spot for throughput + quality
- still relatively heavy

### 4-bit (GGUF / GPTQ / AWQ-ish ecosystems)

- the reason consumer GPUs can run surprisingly large models
- quality is often good enough for chat + tooling
- edge cases appear first in long reasoning, strict formatting, multilingual

You‚Äôre not buying ‚Äú4-bit‚Äù. You‚Äôre buying:

- a quantization scheme
- a kernel implementation
- a runtime

Those three decide your experience.

## Step 4 ‚Äî runtimes: GGUF vs vLLM vs TGI

### GGUF (llama.cpp ecosystem)

Use when:

- you want reliability
- you‚Äôre on consumer hardware
- you want good CPU fallback / hybrid execution

Trade-offs:

- peak throughput lower than optimized server stacks
- multi-GPU story depends on backend and model

### vLLM

Use when:

- you want high throughput serving
- you need good batching + paged attention
- you‚Äôre okay with a more ‚Äúserver‚Äù mindset

Trade-offs:

- more moving parts
- GPU-centric

### TGI (Text Generation Inference)

Use when:

- you want a mature production-oriented server
- you want good observability patterns

Trade-offs:

- operational overhead compared to local single-binary solutions

## Step 5 ‚Äî throughput vs latency: you‚Äôre on a frontier

Batching is the most effective way to get throughput.

But it moves you along a frontier: better throughput, worse tail latency.

Toy example (shapes matter more than values):

![](/blog/2026-02-10-local-llm-setup/throughput-latency-frontier.svg)

How to operationalize this:

- interactive mode: cap batching time (e.g. 10‚Äì30ms)
- batch mode: maximize utilization, accept latency
- mixed mode: separate queues / replicas

## Step 6 ‚Äî LoRA fine-tuning basics (just enough)

LoRA fine-tuning is a way to adapt a model without updating all weights.

The practical benefits:

- lower VRAM requirement than full fine-tune
- faster iterations
- easy to maintain multiple adapters

The practical traps:

- garbage in, garbage out (dataset quality dominates)
- evaluation is often missing (people ‚Äúfeel‚Äù improvements)
- you can overfit formatting without improving capability

A good local workflow:

1. pick a narrow task (classification, extraction, style)
2. build a small, clean dataset
3. track a held-out set
4. test under the *serving runtime* you‚Äôll actually deploy

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a worked example: ‚ÄúGiven 24GB VRAM, what model + context + concurrency fits?‚Äù
- Add notes on offload: when CPU offload is okay (spoiler: batch jobs) vs painful (interactive).
