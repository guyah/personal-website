---
title: "GPUs, TPUs, and accelerators: the mental model that survives benchmarks"
description: "Architecture basics (NVIDIA-ish GPUs), memory hierarchy, tensor cores, batching, interconnects ‚Äî and what they imply for training vs serving."
pubDate: 2026-02-09
tags: [systems, llm, hardware, gpu, inference]
icon: "üß†"
draft: true
---

Benchmarks are a terrible way to build intuition.

They compress *architecture* (memory hierarchy, scheduling, interconnects) into a single number, then we argue about the number.

This post is the mental model I wish I had before I started buying GPUs ‚Äúbecause they‚Äôre fast‚Äù.

Plots generated by:

- `site/analysis/generate_all.py` (task: `gpus_vs_tpus`)

## The one sentence model

An accelerator is a machine for turning **lots of independent, uniform math** into **high utilization**.

Everything else‚Äîtensor cores, systolic arrays, HBM, NVLink‚Äîexists to answer two questions:

1. **How do we keep the compute units fed?** (memory, caching, scheduling)
2. **How do we keep many chips acting like one?** (interconnect, collectives)

If you internalize those, the rest is detail.

## GPU vs TPU (and friends): what differs *architecturally*

### GPUs (general): flexible parallelism

A modern NVIDIA-ish GPU is:

- many **SMs** (streaming multiprocessors)
- each SM has schedulers, register files, shared memory / L1
- a **memory subsystem** (L2 + HBM) that is *fast* but still far slower than on-chip storage
- special datapaths for matrix math (**tensor cores**) that provide huge throughput *if* you feed them well

GPU strengths:

- flexible kernels (not just matmuls)
- good for ‚Äúweird‚Äù models and custom ops
- good ecosystem tooling

GPU traps:

- you can be compute-rich and still slow because you‚Äôre **memory-bound**
- you can have lots of FLOPs and still be slow because you‚Äôre **launch / sync / Python-bound**

### TPUs (general): structured throughput

TPUs (and TPU-like designs) trade flexibility for:

- predictable matrix execution (often **systolic** or similarly structured)
- strong compiler stack + static graph assumptions
- a bias toward large matmul-heavy workloads

TPU strengths:

- excellent throughput on ‚Äúhappy path‚Äù linear algebra
- good scaling inside the TPU fabric

TPU traps:

- less forgiving when your workload has irregular control flow or lots of small, varied kernels
- compiler and runtime behavior can dominate your life

### ‚ÄúOther accelerators‚Äù (NPU, inference ASICs)

Most inference accelerators are just taking the same bet:

- less generality
- more fixed-function scheduling + memory layout
- better perf/W on a narrower workload

If you know what your production workload is *and it doesn‚Äôt change much*, these can win.

If you‚Äôre still experimenting, you may be buying constraints.

## The thing that actually decides performance: the memory hierarchy

Compute is cheap. Data movement is expensive.

The reason ‚ÄúGPU programming‚Äù feels like wizardry is that you‚Äôre not programming FLOPs‚Äîyou‚Äôre programming **where the bytes live**.

![](/blog/2026-02-09-gpus-vs-tpus/memory-hierarchy.svg)

Practical implications:

- If you can reuse values in registers/shared memory, you win.
- If you bounce to HBM every operation, you lose.
- If you spill to host RAM over PCIe, you‚Äôre doing *distributed systems* now.

### Tensor cores are not magic

Tensor cores are just a very fast datapath for a particular set of math shapes.

They don‚Äôt fix:

- bad memory access patterns
- tiny batch sizes
- kernels dominated by non-matmul ops

What they *do* change is: when your math is compatible, the performance ceiling is much higher. That makes memory/feed even more important.

## Batching: the utilization lever (and the latency tax)

For serving, you usually have two knobs:

- **batch size** (how many requests you combine)
- **sequence length / context** (which drives KV cache)

Batching typically increases throughput until you saturate:

- compute
- memory bandwidth
- KV cache pressure
- scheduling overhead

A toy curve (shape is the point):

![](/blog/2026-02-09-gpus-vs-tpus/throughput-vs-batch.svg)

The serving reality:

- small batches ‚Üí low latency, low utilization
- large batches ‚Üí high utilization, but queueing/p99 latency can get ugly

For production you want a **policy** (dynamic batching with time caps) rather than a fixed batch size.

## Interconnects: when one GPU is not enough

The minute your model doesn‚Äôt fit, or your throughput needs exceed one device, you care about:

- bandwidth
- latency
- topology
- collective performance (all-reduce, all-gather)

Illustrative bandwidth orders of magnitude:

![](/blog/2026-02-09-gpus-vs-tpus/interconnect-bandwidth.svg)

The serving implications are subtle:

- **Tensor parallelism** is interconnect-hungry (lots of cross-device traffic).
- **Pipeline parallelism** is latency-hungry (bubbles + microbatching).
- **Data parallelism** is training-friendly; for serving it‚Äôs usually just replication.

If your interconnect is weak, you‚Äôll often do better with:

- smaller models replicated more times
- aggressive quantization
- fewer cross-device boundaries

## How to choose (without vibes)

Ask these questions in order:

1. **Is the workload matmul-dominant and stable?** If yes, specialized accelerators become viable.
2. **Do you need custom ops / rapid iteration?** If yes, GPU ecosystem wins.
3. **Is the bottleneck memory bandwidth, or compute?** Profiling beats speculation.
4. **Will you scale across devices?** If yes, interconnect matters as much as FLOPs.
5. **Is this training, fine-tuning, or serving?** They stress different parts of the system.

A boring but useful rule:

> The best accelerator is the one that keeps utilization high *with the least operational complexity*.

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a roofline-style sketch (compute vs bandwidth) using a real GPU spec sheet.
- Add a section on KV cache: why long context is ‚Äúmemory bandwidth + capacity‚Äù more than FLOPs.
