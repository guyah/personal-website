---
title: "Where WER hides: noise, domain shift, and the long tail"
description: "Word Error Rate is an average that often lies. Look at SNR curves and utterance-level distributions to see what users actually experience."
pubDate: 2026-02-07
tags: [asr, voice, evaluation, data-science]
icon: "üìù"
---

WER (Word Error Rate) is the default ASR metric.

It‚Äôs also one of the easiest metrics to misunderstand.

The mistake is treating ‚ÄúWER = 8%‚Äù as if it implies a smooth user experience.

In practice, ASR failures are often dominated by:

- **domain shift** (names, jargon, accents, products)
- **noise and microphone differences**
- **a long tail of catastrophic utterances**

This post shows two plots I like for making that visible.

Plots generated by:

- `site/analysis/generate_all.py` (task: `asr_wer`)

## WER vs noise (SNR)

A curve over signal-to-noise ratio immediately answers:

- how robust are we to noise?
- is there an irreducible floor?

Synthetic example (shape is the point):

![](/blog/2026-02-07-asr-wer/wer-vs-snr.svg)

Domain shift often shows up as:

- worse WER at every SNR
- and/or a worse *floor* even at high SNR

Because it‚Äôs not noise; it‚Äôs vocabulary and priors.

## Utterance-level WER distribution (the long tail)

Users don‚Äôt experience average WER.

They experience **per-utterance** outcomes:

- most utterances are fine
- a few are disasters

Synthetic heavy-tailed utterance WER:

![](/blog/2026-02-07-asr-wer/utterance-wer-hist.svg)

This is why voice products can feel broken even with decent average WER.

### Practical implication

You need tooling for tail cases:

- fallback confirmations
- robust slot filling
- spelling / name capture flows
- retry prompts that don‚Äôt feel accusatory

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add stratified WER by intent type (commands vs dictation).
- Add confidence calibration plots (reliability diagram for ASR confidence).
