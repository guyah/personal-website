---
title: "Sampling controls without folklore: temperature, top-p, and why â€˜creativeâ€™ isnâ€™t a setting"
description: "A data-science view of decoding controls: entropy, nucleus truncation, and how small changes shift probability mass."
pubDate: 2026-01-31
tags: [llm, transformers, inference, data-science]
icon: "ğŸ›"
---

When people say *â€œturn the temperature upâ€* theyâ€™re usually asking for something like:

- more surprising word choice
- less repetition
- a wider spread of ideas

The problem is: those things are related, but not identical.

This post is a decoding-control sanity map: what each knob does, what it canâ€™t do, and the simplest plots I know that make the behavior stick.

All plots in this post are generated by:

- `site/analysis/generate_all.py` (task: `sampling_controls`)

## The only thing sampling touches: a probability distribution

At each generation step you have a categorical distribution over the next token.

- logits â†’ softmax â†’ probabilities
- then you sample (or greedily pick max)

Everything below is about reshaping that distribution *before* sampling.

## Temperature: a knob for entropy

Temperature rescales logits:

- lower T â†’ more peaky distribution
- higher T â†’ flatter distribution

In a toy next-token distribution, you can see entropy rise with temperature:

![](/blog/2026-01-31-sampling-controls/entropy-vs-temperature.svg)

### Practical implication

- If your model is already uncertain, increasing temperature mainly increases noise.
- If your model is confident-but-boring, increasing temperature can help explore alternatives.

The mistake is thinking temperature â€œadds creativity.â€ It increases the probability of *non-argmax* tokens; whether that feels creative depends on your base distribution.

## Top-p: truncating the tail (nucleus sampling)

Top-p (aka nucleus sampling) keeps the smallest set of tokens whose cumulative probability â‰¥ p.

In a toy distribution, the CDF curve looks like this:

![](/blog/2026-01-31-sampling-controls/top-p-cdf.svg)

The key idea:

- With a **peaky** distribution, top-p keeps only a handful of tokens.
- With a **flat** distribution, top-p keeps a lot.

So top-p behaves differently across contexts. It is *not* a constant â€œcreativity slider.â€

## A simple workflow thatâ€™s actually debuggable

When Iâ€™m tuning a production system, I try to separate goals:

- **reduce repetition** â†’ repetition penalties, frequency penalties, stopword handling, better prompting
- **increase diversity** â†’ slightly higher temperature, slightly higher top-p
- **increase factuality** â†’ *not decoding*; do retrieval, citations, constrained generation, verification

The most common failure mode is using decoding controls to paper over a data / task mismatch.

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a section on *beam search* (and why it often makes prose worse).
- Add an empirical â€œdiversity vs factualityâ€ curve on a real dataset.
