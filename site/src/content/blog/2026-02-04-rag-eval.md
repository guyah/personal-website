---
title: "RAG without vibes: evaluate retrieval, then evaluate answering"
description: "A small, practical metrics stack for retrieval-augmented generation: precision@k, recall@k, and answerability gating."
pubDate: 2026-02-04
tags: [llm, rag, evaluation, data-science]
icon: "ğŸ“š"
---

Most RAG failures are diagnosed backwards.

People watch a bad answer and say: *â€œthe model hallucinated.â€*

But the real questions are:

1. **Did retrieval return the right stuff?**
2. **If retrieval was good, did the model use it?**
3. **If retrieval was bad, did we have a mechanism to refuse?**

This post is a metrics-first checklist.

Plots generated by:

- `site/analysis/generate_all.py` (task: `rag_eval`)

## Step 1 â€” retrieval metrics: precision@k and recall@k

If you label relevance, you can compute:

- **precision@k**: of the k retrieved chunks, what fraction are relevant?
- **recall@k**: of all relevant chunks, what fraction did we retrieve?

A synthetic example (the shape matters more than the numbers):

![](/blog/2026-02-04-rag-eval/precision-recall-at-k.svg)

What to learn from the curve:

- If precision@k is low, youâ€™re feeding the model noise.
- If recall@k is low, youâ€™re starving it of key facts.
- Increasing k usually increases recall and decreases precision.

Thatâ€™s not a bug; itâ€™s the core trade-off.

## Step 2 â€” answerability gating

Even with a good retriever, some questions are unanswerable from your corpus.

A robust system needs a way to say:

> â€œI canâ€™t answer this from the sources I have.â€

You can treat â€œanswerable vs notâ€ as a classification problem based on a **grounding score** (e.g., max similarity, entailment score, citation coverage).

A ROC-like trade-off curve (synthetic):

![](/blog/2026-02-04-rag-eval/answerability-roc.svg)

If you operate at a high-recall point (high TPR), you accept more false positivesâ€”meaning youâ€™ll attempt answers you shouldnâ€™t.

If you operate at a low-FPR point, you refuse more often.

Neither is â€œcorrect.â€ Itâ€™s a product decision.

## Step 3 â€” answer evaluation (separate from retrieval)

Once retrieval is measurable, answering evaluation becomes cleaner:

- citation coverage
- contradiction checks
- claim extraction + verification
- human eval focused on synthesis quality

The key is **not mixing the failure modes**.

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a minimal labeling schema for relevance (what counts as relevant?).
- Add a â€œcitation densityâ€ metric and show it correlates with correctness.
