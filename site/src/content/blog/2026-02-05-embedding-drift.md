---
title: "Monitoring embedding drift in production"
description: "A practical drift playbook: visualize, score, alert, and connect drift back to product failures."
pubDate: 2026-02-05
tags: [ml, embeddings, monitoring, data-science]
icon: "ðŸ§­"
---

If you ship embeddings, you ship a moving target.

Even if the embedding model is fixed:

- user behavior shifts
- content shifts
- language shifts
- product UX shifts (which changes what people type)

If you donâ€™t measure drift, you will eventually debug a â€œmysterious retrieval regressionâ€ at 2am.

This post is a minimal, reproducible pattern.

Plots generated by:

- `site/analysis/generate_all.py` (task: `embedding_drift`)

## Step 1 â€” visualize drift (PCA is enough to start)

You donâ€™t need perfect dimensionality reduction. You need *a picture*.

A synthetic example: week 0 embeddings vs week 4 embeddings projected to 2D via PCA:

![](/blog/2026-02-05-embedding-drift/embedding-drift-pca.svg)

What to look for:

- mean shifts
- spread changes
- emergence of new clusters

## Step 2 â€” quantify drift

Visualization is necessary but not sufficient.

A simple approach is to track a drift score on a stable projection (e.g., PC1):

- histogram week 0
- histogram week 4

![](/blog/2026-02-05-embedding-drift/pc1-week0.svg)

![](/blog/2026-02-05-embedding-drift/pc1-week4.svg)

In production youâ€™d replace â€œeyeballing histogramsâ€ with a metric:

- energy distance
- Wasserstein distance
- MMD
- KL (with care)

The exact choice matters less than having a consistent signal.

## Step 3 â€” connect drift to outcomes

Drift only matters if it breaks something.

So pair drift metrics with:

- retrieval success rate
- click-through / satisfaction
- â€œno answerâ€ rate
- escalation rate

Then ask:

> When drift spikes, which product metrics degrade?

## Reproduce

```bash
python site/analysis/generate_all.py
```

### TODO

- Add a streaming-friendly drift metric (sketch / reservoir sampling).
- Add a template dashboard (Prometheus-style) for drift + outcomes.
